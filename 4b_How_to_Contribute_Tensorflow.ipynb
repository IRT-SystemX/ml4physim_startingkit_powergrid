{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Run of an Augmented Simulator (Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial notebook provides a guidance for installing the required packages and testing implemented augmented simulators using LIPS platform. \n",
    "\n",
    "**A quick walkthrough:**\n",
    "\n",
    "- Install the required packages using the requirements.txt file in the github repository for the required used case.\n",
    "\n",
    "- Some baseline are already implemented in the LIPS platform that could be seen to have some inspiration.\n",
    "\n",
    "- The augmented simulators related hyperparameters could be modified via dedicated configuration files.\n",
    "\n",
    "- The LIPS platform will be used to evaluate the trained augmented simulators from different evaluation criteria categories and attribute a score to each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement your own augmented simulator\n",
    "\n",
    "In the following, we show 3 ways to implement an augmented simulator (based on ML or a hybrid physics-AI model):\n",
    "\n",
    "1- Using an existing augmented simulator (baseline) in LIPS platform, train it and then evaluate the results;\n",
    "\n",
    "2- Implement an augmented simulator using LIPS framework template to take the advantage of existing training loop and other offered features;\n",
    "\n",
    "3- Implement an augmented simulator independently from LIPS platform and plug the trained model into LIPS to evaluate its results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As so, this notebook is organized as follows:\n",
    "1. [Generic step (Load the required data)](#generic_step)\n",
    "2. [Evaluate an existing augmented simulator](#existing_sim) (Beginner users)\n",
    "3. [Train and evaluate a custom augmented simulators developed using LIPS framework](#train_using_lips) (Intermediate level users)\n",
    "4. [Train a custom augmented simulator independently from LIPS and use the framwork to evaluate the final results](#train_custom) (Advanced users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For developments on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install a virtual environment\n",
    "# Option 1:  using conda (recommended)\n",
    "!conda create -n venv_lips python=3.10\n",
    "!conda activate venv_lips\n",
    "\n",
    "# Option 2: using virtualenv\n",
    "!pip install virtualenv\n",
    "!virtualenv -p /usr/bin/python3.10 venv_lips\n",
    "!source venv_lips/bin/activate\n",
    "\n",
    "### Install the LIPS framework\n",
    "# Option 1: Get the last version of LIPS framework from PyPI (Recommended)\n",
    "!pip install 'lips-benchmark[recommended]'\n",
    "\n",
    "# Option 2: Get the last version from github repository\n",
    "!git clone https://github.com/IRT-SystemX/LIPS.git\n",
    "!pip install -U LIPS/.[recommended]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Google Colab Users\n",
    "You could also use a GPU device from `Runtime > Change runtime type` and by selecting `T4 GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install the LIPS framework\n",
    "# Option 1: Get the last version of LIPS framework from PyPI (Recommended)\n",
    "!pip install 'lips-benchmark[recommended]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Get the last version from github repository\n",
    "!git clone https://github.com/IRT-SystemX/LIPS.git\n",
    "!pip install -U LIPS/.[recommended]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: You may restart the session after this installation, in order that the changes be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the starting kit\n",
    "!git clone https://github.com/IRT-SystemX/ml4physim_startingkit_powergrid.git\n",
    "# and change the directory to the starting kit to be able to run correctly this notebook\n",
    "import os\n",
    "os.chdir(\"ml4physim_startingkit_powergrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset\n",
    "\n",
    "The already provided datasets on starting kit are demo versions of the complet datasets. The complet datasets should be downloaded using the following function and replace the demo versions.\n",
    "\n",
    "**NB.** <span style=\"color: red\">The challenge dataset is based on `lips_idf_2023` environment and all the solutions should be trained and evaluated on this dataset.</span> Execution of the following cell will replace the demo dataset with the complet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the dataset through the dedicated lips function\n",
    "from lips.dataset.powergridDataSet import downloadPowergridDataset\n",
    "\n",
    "downloadPowergridDataset(\"input_data_local\", \"lips_idf_2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some required pathes\n",
    "import pathlib\n",
    "import os\n",
    "DATA_PATH = pathlib.Path().resolve() / \"input_data_local\" / \"lips_idf_2023\"\n",
    "BENCH_CONFIG_PATH = pathlib.Path().resolve() / \"configs\" / \"benchmarks\" / \"lips_idf_2023.ini\"\n",
    "SIM_CONFIG_PATH = pathlib.Path().resolve() / \"configs\" / \"simulators\"\n",
    "TRAINED_MODELS = pathlib.Path().resolve() / \"input_data_local\" / \"trained_models\"\n",
    "LOG_PATH = \"logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n",
    "\n",
    "1. Ease the importing of datasets\n",
    "1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required benchmark datasets\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark\n",
    "\n",
    "benchmark_kwargs = {\"attr_x\": (\"prod_p\", \"prod_v\", \"load_p\", \"load_q\"),\n",
    "                    \"attr_y\": (\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"v_or\", \"v_ex\"),\n",
    "                    \"attr_tau\": (\"line_status\", \"topo_vect\"),\n",
    "                    \"attr_physics\": None}\n",
    "\n",
    "benchmark = PowerGridBenchmark(benchmark_name=\"Benchmark_competition\",\n",
    "                               benchmark_path=DATA_PATH,\n",
    "                               load_data_set=True,\n",
    "                               log_path=None,\n",
    "                               config_path=BENCH_CONFIG_PATH,\n",
    "                               **benchmark_kwargs\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow users may use the commands below to select a GPU from the available physical devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set at program startup\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the list of selected devices\n",
    "tf.config.experimental.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option-I (Evaluate an existing augmented simulator) <a id='existing_sim'></a>\n",
    "**<font color='red'>For beginners.</font>**\n",
    "\n",
    "We start by importing an architecture from exisiting set of architectures and instantiate the `TfFullyConnected` class which offers a set of utilities to train and analyze the selected augmented simulator. User could play with the configuration file of an existing augmented simulator to modify the model hyperparameters.\n",
    "\n",
    "The configuration file could be found at `./configurations/airfoil/simulators/tf_fc.ini`:\n",
    "\n",
    "```output\n",
    "[DEFAULT]\n",
    "name = \"tf_fc\"\n",
    "layers = (300, 300, 300, 300)\n",
    "activation = \"relu\"\n",
    "layer = \"linear\"\n",
    "input_dropout = 0.0\n",
    "dropout = 0.0\n",
    "metrics = [\"mae\"]\n",
    "loss = {\"name\": \"mse\",\n",
    "        \"params\": {\"size_average\": None,\n",
    "                   \"reduce\": None,\n",
    "                   \"reduction\": 'mean'}}\n",
    "device = \"cpu\"\n",
    "optimizer = {\"name\": \"adam\",\n",
    "             \"params\": {\"lr\": 3e-4}}\n",
    "train_batch_size = 128\n",
    "eval_batch_size = 128\n",
    "epochs = 5\n",
    "shuffle = True\n",
    "save_freq = False\n",
    "ckpt_freq = 50\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we select the configuration provided in `[DEFAULT]` section and new configuration could be created using a new section name and modifying the existing parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we select the fully connected architecture with its corresponding configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_fc.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark_competition\",\n",
    "                         bench_kwargs=benchmark_kwargs,\n",
    "                         sim_config_path=SIM_CONFIG_PATH,\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the augmented simulator using the benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.train(train_dataset=benchmark.train_dataset,\n",
    "            val_dataset=benchmark.val_dataset,\n",
    "            epochs=10\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Load\n",
    "You can also save and load the model fitted parameters alongside its meta data using the following functions.\n",
    "\n",
    "Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = TRAINED_MODELS / \"fully_connected\"\n",
    "tf_fc.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your trained augmented simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark_competition\",\n",
    "                         bench_kwargs=benchmark_kwargs,\n",
    "                         sim_config_path=SIM_CONFIG_PATH,\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)\n",
    "\n",
    "LOAD_PATH = TRAINED_MODELS / \"fully_connected\"\n",
    "tf_fc.restore(path=LOAD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the trained augmented simulator could be evaluated using the `evaluate_simulator` function of the `Benchmark` class. You can set on which dataset you want to evaluate your trained augmented simulator. The possibilites are `all`, `val`, `test`, `test_ood_topo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL_SAVE_PATH = get_path(EVALUATION_PATH, benchmark1)\n",
    "tf_sim_metrics = benchmark.evaluate_simulator(augmented_simulator=tf_fc,\n",
    "                                              eval_batch_size=100000,\n",
    "                                              dataset=\"all\",\n",
    "                                              shuffle=False,\n",
    "                                              save_path=None,\n",
    "                                              save_predictions=False\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how your model performs directly by looking at the evaluation metrics resulted by from the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_sim_metrics[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option-II (Train and evaluate a new augmented simulator using LIPS platform) <a id='train_using_lips'></a>\n",
    "\n",
    "**<font color='red'>For intermediate level users.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensorflow based fully connected\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Union\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# from leap_net import ResNetLayer\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    from tensorflow import keras\n",
    "\n",
    "from lips.augmented_simulators.tensorflow_simulator import TensorflowSimulator\n",
    "from lips.logger import CustomLogger\n",
    "from lips.config import ConfigManager\n",
    "from lips.dataset import DataSet\n",
    "from lips.dataset.scaler import Scaler\n",
    "from lips.utils import NpEncoder\n",
    "\n",
    "class MyCustomFullyConnected(TensorflowSimulator):\n",
    "    \"\"\"Fully Connected architecture\n",
    "    Parameters\n",
    "    ----------\n",
    "    sim_config_path : ``str``\n",
    "        The path to the configuration file for simulator.\n",
    "        It should contain all the required hyperparameters for this model.\n",
    "    sim_config_name : Union[str, None], optional\n",
    "        _description_, by default None\n",
    "    name : Union[str, None], optional\n",
    "        _description_, by default None\n",
    "    scaler : Union[Scaler, None], optional\n",
    "        _description_, by default None\n",
    "    bench_config_path : Union[str, pathlib.Path, None], optional\n",
    "        _description_, by default None\n",
    "    bench_config_name : Union[str, None], optional\n",
    "        _description_, by default None\n",
    "    log_path : Union[None, str], optional\n",
    "        _description_, by default None\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 sim_config_path: str,\n",
    "                 bench_config_path: Union[str, pathlib.Path],\n",
    "                 bench_config_name: Union[str, None]=None,\n",
    "                 bench_kwargs: dict={},\n",
    "                 sim_config_name: Union[str, None]=None,\n",
    "                 name: Union[str, None]=None,\n",
    "                 scaler: Union[Scaler, None]=None,\n",
    "                 log_path: Union[None, str]=None,\n",
    "                 **kwargs):\n",
    "        super().__init__(name=name, log_path=log_path, **kwargs)\n",
    "        if not os.path.exists(sim_config_path):\n",
    "            raise RuntimeError(\"Configuration path for the simulator not found!\")\n",
    "        if not str(sim_config_path).endswith(\".ini\"):\n",
    "            raise RuntimeError(\"The configuration file should have `.ini` extension!\")\n",
    "        sim_config_name = sim_config_name if sim_config_name is not None else \"DEFAULT\"\n",
    "        self.sim_config = ConfigManager(section_name=sim_config_name, path=sim_config_path)\n",
    "        self.bench_config = ConfigManager(section_name=bench_config_name, path=bench_config_path)\n",
    "        self.bench_config.set_options_from_dict(**bench_kwargs)\n",
    "        self.name = name if name is not None else self.sim_config.get_option(\"name\")\n",
    "        self.name = self.name + '_' + sim_config_name\n",
    "        # scaler\n",
    "        self.scaler = scaler() if scaler else None\n",
    "        # Logger\n",
    "        self.log_path = log_path\n",
    "        self.logger = CustomLogger(__class__.__name__, log_path).logger\n",
    "        # model parameters\n",
    "        self.params = self.sim_config.get_options_dict()\n",
    "        self.params.update(kwargs)\n",
    "        # Define layer to be used for the model\n",
    "        self.layers = {\"linear\": keras.layers.Dense}#, \"resnet\" : ResNetLayer}\n",
    "        self.layer = self.layers.get(self.params[\"layer\"], None)\n",
    "        if self.layer is None:\n",
    "            self.layer = keras.layers.Dense \n",
    "\n",
    "        # optimizer\n",
    "        if \"optimizer\" in kwargs:\n",
    "            if not isinstance(kwargs[\"optimizer\"], keras.optimizers.Optimizer):\n",
    "                raise RuntimeError(\"If an optimizer is provided, it should be a type tensorflow.keras.optimizers\")\n",
    "            self._optimizer = kwargs[\"optimizer\"](self.params[\"optimizer\"][\"params\"])\n",
    "        else:\n",
    "            self._optimizer = keras.optimizers.Adam(learning_rate=self.params[\"optimizer\"][\"params\"][\"lr\"])\n",
    "\n",
    "        self._model: Union[keras.Model, None] = None\n",
    "\n",
    "        self.input_size = None if kwargs.get(\"input_size\") is None else kwargs[\"input_size\"]\n",
    "        self.output_size = None if kwargs.get(\"output_size\") is None else kwargs[\"output_size\"]\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build the model\n",
    "        Returns\n",
    "        -------\n",
    "        Model\n",
    "            _description_\n",
    "        \"\"\"\n",
    "        super().build_model()\n",
    "        input_ = keras.layers.Input(shape=(self.input_size,), name=\"input\")\n",
    "        x = input_\n",
    "        x = keras.layers.Dropout(rate=self.params[\"input_dropout\"], name=\"input_dropout\")(x)\n",
    "        for layer_id, layer_size in enumerate(self.params[\"layers\"]):\n",
    "            x = self.layer(layer_size, name=f\"layer_{layer_id}\")(x)\n",
    "            x = keras.layers.Activation(self.params[\"activation\"], name=f\"activation_{layer_id}\")(x)\n",
    "            x = keras.layers.Dropout(rate=self.params[\"dropout\"], name=f\"dropout_{layer_id}\")(x)\n",
    "        output_ = keras.layers.Dense(self.output_size)(x)\n",
    "        self._model = keras.Model(inputs=input_,\n",
    "                                  outputs=output_,\n",
    "                                  name=f\"{self.name}_model\")\n",
    "        return self._model\n",
    "\n",
    "    def process_dataset(self, dataset: DataSet, training: bool=False) -> tuple:\n",
    "        \"\"\"process the datasets for training and evaluation\n",
    "        This function transforms all the dataset into something that can be used by the neural network (for example)\n",
    "        Warning\n",
    "        -------\n",
    "        It works with StandardScaler only for the moment.\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : DataSet\n",
    "            _description_\n",
    "        Scaler : bool, optional\n",
    "            _description_, by default True\n",
    "        training : bool, optional\n",
    "            _description_, by default False\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            the normalized dataset with features and labels\n",
    "        \"\"\"\n",
    "        if training:\n",
    "            self._infer_size(dataset)\n",
    "            inputs, outputs = dataset.extract_data(concat=True)\n",
    "            if self.scaler is not None:\n",
    "                inputs, outputs = self.scaler.fit_transform(inputs, outputs)\n",
    "        else:\n",
    "            inputs, outputs = dataset.extract_data(concat=True)\n",
    "            if self.scaler is not None:\n",
    "                inputs, outputs = self.scaler.transform(inputs, outputs)\n",
    "\n",
    "        return inputs, outputs\n",
    "\n",
    "    def _infer_size(self, dataset: DataSet):\n",
    "        \"\"\"Infer the size of the model\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : DataSet\n",
    "            _description_\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "            _description_\n",
    "        \"\"\"\n",
    "        *dim_inputs, self.output_size = dataset.get_sizes()\n",
    "        self.input_size = np.sum(dim_inputs)\n",
    "\n",
    "    def _post_process(self, dataset, predictions):\n",
    "        if self.scaler is not None:\n",
    "            predictions = self.scaler.inverse_transform(predictions)\n",
    "        predictions = super()._post_process(dataset, predictions)\n",
    "        return predictions\n",
    "\n",
    "    def _save_metadata(self, path: str):\n",
    "        super()._save_metadata(path)\n",
    "        if self.scaler is not None:\n",
    "            self.scaler.save(path)\n",
    "        res_json = {}\n",
    "        res_json[\"input_size\"] = self.input_size\n",
    "        res_json[\"output_size\"] = self.output_size\n",
    "        with open((path / \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(obj=res_json, fp=f, indent=4, sort_keys=True, cls=NpEncoder)\n",
    "\n",
    "    def _load_metadata(self, path: str):\n",
    "        if not isinstance(path, pathlib.Path):\n",
    "            path = pathlib.Path(path)\n",
    "        super()._load_metadata(path)\n",
    "        if self.scaler is not None:\n",
    "            self.scaler.load(path)\n",
    "        with open((path / \"metadata.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            res_json = json.load(fp=f)\n",
    "        self.input_size = res_json[\"input_size\"]\n",
    "        self.output_size = res_json[\"output_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, the augmented simulator is implemented, you should also create a configuration which indicate all the hyper parameters required by this augmented simulator. An example of configuration file is shown in `configs/simulators/tf_fc.ini` and its content is shown below. \n",
    "\n",
    "The path and the section name of this configuration file should be given to your architecture as an argument (`sim_config_path`, `sim_config_name`) in order that it could be able to import all its required hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```output\n",
    "[DEFAULT]\n",
    "name = \"tf_fc\"\n",
    "layers = (300, 300, 300, 300)\n",
    "activation = \"relu\"\n",
    "layer = \"linear\"\n",
    "input_dropout = 0.0\n",
    "dropout = 0.0\n",
    "metrics = [\"mae\"]\n",
    "loss = {\"name\": \"mse\",\n",
    "        \"params\": {\"size_average\": None,\n",
    "                   \"reduce\": None,\n",
    "                   \"reduction\": 'mean'}}\n",
    "device = \"cpu\"\n",
    "optimizer = {\"name\": \"adam\",\n",
    "             \"params\": {\"lr\": 3e-4}}\n",
    "train_batch_size = 128\n",
    "eval_batch_size = 128\n",
    "epochs = 5\n",
    "shuffle = True\n",
    "save_freq = False\n",
    "ckpt_freq = 50\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate your simulator with corresponding configurations and a scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_fc.ini\")\n",
    "\n",
    "# Import a scaler\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "custom_tf_fc = MyCustomFullyConnected(name=\"tf_fc\",\n",
    "                                      bench_config_path=BENCH_CONFIG_PATH,\n",
    "                                      bench_config_name=\"Benchmark_competition\",\n",
    "                                      bench_kwargs=benchmark_kwargs,\n",
    "                                      sim_config_path=SIM_CONFIG_PATH,\n",
    "                                      sim_config_name=\"DEFAULT\",\n",
    "                                      scaler=StandardScaler,\n",
    "                                      log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train` function is implemented in the base class `TensorflowSimulator`. You can call it directly to train your custom augmented simulator. You can also overload it and define your own training function if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tf_fc.train(train_dataset=benchmark.train_dataset,\n",
    "                   val_dataset=benchmark.val_dataset,\n",
    "                   epochs=5\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can train your custom augmented simualator using the evaluation module of LIPS framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL_SAVE_PATH = get_path(EVALUATION_PATH, benchmark1)\n",
    "custom_fc_metrics = benchmark.evaluate_simulator(augmented_simulator=custom_tf_fc,\n",
    "                                                 eval_batch_size=100000,\n",
    "                                                 dataset=\"all\",\n",
    "                                                 shuffle=False,\n",
    "                                                 save_path=None,\n",
    "                                                 save_predictions=False\n",
    "                                                ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option-III (Train an augmented simulator independently and evaluate it through LIPS) <a id='train_custom'></a>\n",
    "\n",
    "**<font color='red'>For advanced users.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you requrie more functionalities that are not offered by LIPS platform (e.g., adding advanced regularizations into the training loop, or adding physics constraints in your model) you can implement your architecture independently from LIPS platform and use only the evaluation part of the framework to assess your model performance. \n",
    "\n",
    "In the following, we show a simple architecture with a training loop and how it can be evaluated by the LIPS platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions required to proprocess the data and post process the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lips.dataset import DataSet\n",
    "\n",
    "def process_dataset(dataset: DataSet, training: bool=False, scaler=None) -> tuple:\n",
    "    if training:\n",
    "        inputs, outputs = dataset.extract_data(concat=True)\n",
    "        if scaler is not None:\n",
    "            inputs, outputs = scaler.fit_transform(inputs, outputs)\n",
    "    else:\n",
    "        inputs, outputs = dataset.extract_data(concat=True)\n",
    "        if scaler is not None:\n",
    "            inputs, outputs = scaler.transform(inputs, outputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "def infer_size(dataset: DataSet):\n",
    "    \"\"\"Infer the size of the model\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DataSet\n",
    "        _description_\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        _description_\n",
    "    \"\"\"\n",
    "    *dim_inputs, output_size = dataset.get_sizes()\n",
    "    input_size = np.sum(dim_inputs)\n",
    "    return input_size, output_size\n",
    "\n",
    "def post_process(dataset, predictions, scaler=None):\n",
    "    if scaler is not None:\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "    predictions = dataset.reconstruct_output(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 1: Implement your architecture based on Tensorflow library in this Example\n",
    "\n",
    "Your class should inherit from `AugmentedSimulator` of LIPS framework and implement the following functions:\n",
    "\n",
    "- `build_model`: design the architecture of the model;\n",
    "- `train`: train the model\n",
    "- `predict`: predict using the trained model\n",
    "\n",
    "**NB.** It is required to respect this format, because the evaluation module gets this class as input and evaluates its inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from lips.augmented_simulators import AugmentedSimulator\n",
    "\n",
    "class MyFullyCustomFullyConnected(AugmentedSimulator):\n",
    "    def __init__(self,\n",
    "                 name: str=\"MyCustomFC\",\n",
    "                 input_size: int=None,\n",
    "                 output_size: int=None,\n",
    "                 hidden_sizes: tuple=(100,100),\n",
    "                 ):\n",
    "        self.name = name\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self._model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        input_ = keras.layers.Input(shape=(self.input_size,), name=\"input\")\n",
    "        x = input_\n",
    "        for layer_id, layer_size in enumerate(self.hidden_sizes):\n",
    "            x = keras.layers.Dense(layer_size, name=f\"layer_{layer_id}\")(x)\n",
    "            x = keras.layers.Activation(\"relu\", name=f\"activation_{layer_id}\")(x)\n",
    "        output_ = keras.layers.Dense(self.output_size)(x)\n",
    "        self._model = keras.Model(inputs=input_,\n",
    "                                  outputs=output_,\n",
    "                                  name=f\"{self.name}_model\")     \n",
    "\n",
    "    def train(self,\n",
    "              train_dataset,\n",
    "              val_dataset,\n",
    "              epochs=10,\n",
    "              lr=3e-4,\n",
    "              shuffle=True,\n",
    "              batch_size=256,\n",
    "              ):\n",
    "        processed_x, processed_y = train_dataset\n",
    "        # init the model\n",
    "        self.build_model()\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self._model.compile(optimizer=optimizer,\n",
    "                            loss=\"mse\",\n",
    "                            metrics=[\"mae\"])        \n",
    "\n",
    "        history_callback = self._model.fit(x=processed_x,\n",
    "                                            y=processed_y,\n",
    "                                            validation_data=val_dataset,\n",
    "                                            epochs=epochs,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=shuffle)\n",
    "        return history_callback\n",
    "\n",
    "    def predict(self, dataset: DataSet, scaler=None, eval_batch_size=128) -> dict:\n",
    "        processed_x, _ = process_dataset(dataset, training=False, scaler=scaler)\n",
    "\n",
    "        # make the predictions\n",
    "        predictions = self._model.predict(processed_x, batch_size=eval_batch_size)\n",
    "\n",
    "        predictions = post_process(dataset, predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "processed_x_train, processed_y_train = process_dataset(benchmark.train_dataset, training=True, scaler=scaler)\n",
    "processed_x_val, processed_y_val = process_dataset(benchmark.val_dataset, training=False, scaler=scaler)\n",
    "training_data = (processed_x_train, processed_y_train)\n",
    "validation_data = (processed_x_val, processed_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = infer_size(benchmark.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyFullyCustomFullyConnected(name=\"MyFullyCustomFC\",\n",
    "                                    input_size=input_size,\n",
    "                                    output_size=output_size,\n",
    "                                    hidden_sizes=(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.train(training_data, validation_data, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prediction on `test_dataset`\n",
    "This dataset has the same distribution as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dataset=benchmark._test_dataset, scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the environment which is required for evaluation\n",
    "from lips.dataset.utils.powergrid_utils import get_kwargs_simulator_scenario\n",
    "from lips.benchmark.powergridBenchmark import get_env\n",
    "\n",
    "env = get_env(get_kwargs_simulator_scenario(benchmark.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.evaluation.powergrid_evaluation import PowerGridEvaluation\n",
    "from pprint import pprint\n",
    "\n",
    "evaluator = PowerGridEvaluation(benchmark.config)\n",
    "metrics_test = evaluator.evaluate(observations=benchmark._test_dataset.data,\n",
    "                                  predictions=predictions,\n",
    "                                  dataset=benchmark._test_dataset,\n",
    "                                  augmented_simulator=model,\n",
    "                                  env=env)\n",
    "pprint(metrics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = dict()\n",
    "metrics_all[\"test\"] = metrics_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction on `test_ood_dataset`\n",
    "This dataset has a different distribution in comparison to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dataset=benchmark._test_ood_topo_dataset, scaler=scaler)\n",
    "\n",
    "evaluator = PowerGridEvaluation(benchmark.config)\n",
    "metrics_ood = evaluator.evaluate(observations=benchmark._test_ood_topo_dataset.data,\n",
    "                                 predictions=predictions,\n",
    "                                 env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all[\"test_ood_topo\"] = metrics_ood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_score import compute_global_score\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    score = compute_global_score(metrics_all, benchmark.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
