{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce the baseline\n",
    "This notebook provides the baseline results on `l2rpn_case14_sandbox` environment obtained through various models (aka augmented simulators) available in LIPS framework. It starts by importing the required dataset, instantiating the benchmark, training an existing augmented simulator and finally it shows how the evaluation is performed to reproduce the baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB. The reference dataset for the competition is not `l2rpn_case14_sandbox` (14 substations) but `l2rpn_idf_2023` (118 substations). We use in this notebook a smaller power grid for faster computations and illustrative purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For developments on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install a virtual environment\n",
    "# Option 1:  using conda (recommended)\n",
    "!conda create -n venv_lips python=3.10\n",
    "!conda activate venv_lips\n",
    "\n",
    "# Option 2: using virtualenv\n",
    "!pip install virtualenv\n",
    "!virtualenv -p /usr/bin/python3.10 venv_lips\n",
    "!source venv_lips/bin/activate\n",
    "\n",
    "### Install the LIPS framework\n",
    "# Option 1: Get the last version of LIPS framework from PyPI (Recommended)\n",
    "!pip install lips-benchmark .[recommended]\n",
    "\n",
    "# Option 2: Get the last version from github repository\n",
    "!git clone https://github.com/IRT-SystemX/LIPS.git\n",
    "!pip install -U LIPS/.[recommended]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Google Colab Users\n",
    "You could also use a GPU device from `Runtime > Change runtime type` and by selecting `T4 GPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install the LIPS framework\n",
    "# Option 1: Get the last version of LIPS framework from PyPI (Recommended)\n",
    "!pip install lips-benchmark .[recommended]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Get the last version from github repository\n",
    "!git clone https://github.com/IRT-SystemX/LIPS.git\n",
    "!pip install -U LIPS/.[recommended]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: You may restart the session after this installation, in order that the changes be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the starting kit\n",
    "!git clone https://github.com/IRT-SystemX/ml4physim_startingkit_powergrid.git\n",
    "# and change the directory to the starting kit to be able to run correctly this notebook\n",
    "import os\n",
    "os.chdir(\"ml4physim_startingkit_powergrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required packages\n",
    "import os\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the required paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_case14_sandbox.ini\")\n",
    "DATA_PATH = os.path.join(\"input_data_local\", \"lips_case14_sandbox\")\n",
    "TRAINED_MODELS = os.path.join(\"input_data_local\", \"trained_models\")\n",
    "LOG_PATH = \"logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset\n",
    "\n",
    "The already provided datasets on starting kit are demo versions of the complet datasets. The complet datasets should be downloaded using the following function and replace the demo versions.\n",
    "\n",
    "**NB.** <span style=\"color: red\">The challenge dataset is based on `lips_idf_2023` environment and all the solutions should be trained and evaluated on this dataset.</span> This notebook illustrates the procedure of reproducing the baseline results for a smaller environment with only 14 nodes. This could be used for new users to learn a little bit more about power grids and flow dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the dataset through the dedicated lips function\n",
    "from lips.dataset.powergridDataSet import downloadPowergridDataset\n",
    "\n",
    "downloadPowergridDataset(\"input_data_local\", \"lips_case14_sandbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "### First step: load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_kwargs = {\"attr_x\": (\"prod_p\", \"prod_v\", \"load_p\", \"load_q\"),\n",
    "                    \"attr_y\": (\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"v_or\", \"v_ex\"),\n",
    "                    \"attr_tau\": (\"line_status\", \"topo_vect\"),\n",
    "                    \"attr_physics\": None}\n",
    "\n",
    "benchmark = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                               config_path=BENCH_CONFIG_PATH,\n",
    "                               benchmark_name=\"Benchmark_competition\",\n",
    "                               load_data_set=True, \n",
    "                               load_ybus_as_sparse=False,\n",
    "                               log_path=LOG_PATH,\n",
    "                               **benchmark_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.train_dataset.data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the benchmark is instantiated, we can verify the corresponding configurations imported from the configuration file indicated in `BENCH_CONFIG_PATH` and `benchmark_name` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.config.get_options_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: Select a model (aka Augmented Simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present the tensorflow based augmented simulators for learning a physical domain. Two different architectures are included for the moment in LIPS framework, which are : \n",
    "- Fully Connected Neural Network\n",
    "- LeapNet Neural network\n",
    "\n",
    "The tensorflow side implementations are a little bit different from torch based implementations. In order to generalize on more architectures, we allow that each model (architecture) be a subclass of `TensorflowSimulator` base class. The main functions to `train`, `evaluate`, `load`, `save` models are implemented in base class and could be used directly by sub-classes without any overloading. Some specific tasks as data preparation and post processing of predictions could be done using the child classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the GPU device if there is one (<span style=\"color:red\">Dont run this cell in Google Colab</span>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "memory_limit = 20000\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required architecture and optionally a scaler used to normalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we select the fully connected architecture with its corresponding configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_fc.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark_competition\",\n",
    "                         bench_kwargs=benchmark_kwargs,\n",
    "                         sim_config_path=SIM_CONFIG_PATH,\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the hyperparameters of the selected architecture by using the `params` attribute. These hyperparameters corresponds to the configuration file indicated by `SIM_CONFIG_PATH` and `sim_config_name` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: Train the augmented simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the inputs and outputs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the process_dataset function is called inside the train function call to prepare the data for training\n",
    "inputs, outputs = tf_fc.process_dataset(benchmark.train_dataset, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 0\n",
    "input_dim = 0\n",
    "for var_name in benchmark_kwargs[\"attr_y\"]:\n",
    "    output_dim += benchmark.train_dataset.data.get(var_name).shape[1]\n",
    "\n",
    "for var_name in (benchmark_kwargs[\"attr_x\"] + benchmark_kwargs[\"attr_tau\"]):\n",
    "    input_dim += benchmark.train_dataset.data.get(var_name).shape[1]\n",
    "\n",
    "print(\"input_dim: \", input_dim)\n",
    "print(\"output_dim: \", output_dim)\n",
    "\n",
    "assert(inputs.shape[1] == input_dim)\n",
    "assert(outputs.shape[1] == output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.train(train_dataset=benchmark.train_dataset,\n",
    "            val_dataset=benchmark.val_dataset,\n",
    "            epochs=2\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save and load the model fitted parameters alongside its meta data using the following functions.\n",
    "\n",
    "Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(TRAINED_MODELS, benchmark.env_name)\n",
    "tf_fc.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_fc.ini\")\n",
    "\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark_competition\",\n",
    "                         bench_kwargs=benchmark_kwargs,\n",
    "                         sim_config_path=SIM_CONFIG_PATH,\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)\n",
    "\n",
    "LOAD_PATH = os.path.join(TRAINED_MODELS, \"fully_connected\")\n",
    "tf_fc.restore(path=LOAD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the convergence of the model using `visualize_convergence` of the augmented simulator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the model (layers and shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth step: Evaluate the augmented simulator\n",
    "In this section, we use the evaluation module of LIPS framework to evaluate the trained augmented simulator. We can see which evaluaton criteria are used to evaluate the performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(benchmark.config.get_option(\"eval_dict\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the augmented simulator, we call simply the `evaluate_simulator` function of the benchmark class, which will be instantiate an evaluation object from the corresponding `PowerGridEvaluation` class and intitialize it with the benchmark configuration file. This function get various arguments:\n",
    "- `augmented_simulator`: Which is the trained augmented simulator;\n",
    "- `eval_batch_size`: the batch size used during the evaluation of the augmented simulator;\n",
    "- `dataset`: a string indicating on which dataset, the evaluation should be performed. The options are `all` for three datasets, `val` for validation dataset only, `test` for test dataset only and `test_ood_topo` for out-of-distribution dataset only.\n",
    "- `shuffle`: whether to shuffle the dataset for the evaluation.\n",
    "- `save_path` and `save_predictions`: parameters allowing to save the evaluation results in indicated path and save the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL_SAVE_PATH = get_path(EVALUATION_PATH, benchmark1)\n",
    "tf_fc_metrics = benchmark.evaluate_simulator(augmented_simulator=tf_fc,\n",
    "                                             eval_batch_size=128,\n",
    "                                             dataset=\"all\",\n",
    "                                             shuffle=False,\n",
    "                                             save_path=None,\n",
    "                                             save_predictions=False\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc_metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc_metrics[\"test\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc_metrics[\"test\"][\"ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc_metrics[\"test\"][\"Physics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_fc_metrics[\"test_ood_topo\"][\"ML\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeapNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to Fully Connected architecture, used in the previous section, where the topology vector (`topo_vect`) and power lines connectivity vector (`line_status`) are used directly as the inputs of the architecture, in LeapNet architecture (see [corresponding article](https://www.sciencedirect.com/science/article/abs/pii/S0925231220305051)) considers the topology vector in latent dimension to take into account the various topology configurations as shown in figure below.\n",
    "\n",
    "![image.png](img/leap_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herein, we get the list of reference topology actions which will be given to LeapNet architecture for their encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_actions = benchmark.config.get_option(\"dataset_create_params\")[\"reference_args\"][\"topo_actions\"]\n",
    "\n",
    "kwargs_tau = []\n",
    "for el in topo_actions:\n",
    "     kwargs_tau.append(el[\"set_bus\"][\"substations_id\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_leapnet.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from lips.augmented_simulators.tensorflow_models.powergrid.leap_net import LeapNet\n",
    "from lips.dataset.scaler.powergrid_scaler import PowerGridScaler\n",
    "\n",
    "leap_net = LeapNet(name=\"tf_leapnet\",                  \n",
    "                   bench_config_path=BENCH_CONFIG_PATH,\n",
    "                   bench_config_name=\"Benchmark_competition\",\n",
    "                   bench_kwargs=benchmark_kwargs,\n",
    "                   sim_config_path=SIM_CONFIG_PATH,\n",
    "                   sim_config_name=\"DEFAULT\", \n",
    "                   log_path=LOG_PATH,\n",
    "                   loss = {\"name\": \"mse\"},\n",
    "                   lr = 1e-4,\n",
    "                   activation = tf.keras.layers.LeakyReLU(alpha=0.01),\n",
    "                   sizes_enc=(),\n",
    "                   sizes_main=(200, 200),\n",
    "                   sizes_out=(),\n",
    "                   topo_vect_to_tau=\"given_list\",\n",
    "                   kwargs_tau = kwargs_tau,\n",
    "                   layer = \"resnet\",\n",
    "                   scale_main_layer = 200,\n",
    "                   scale_input_dec_layer = 200,\n",
    "                   mult_by_zero_lines_pred = False,\n",
    "                   scaler = PowerGridScaler,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net.train(train_dataset=benchmark.train_dataset,\n",
    "               val_dataset=benchmark.val_dataset,\n",
    "               batch_size=256,\n",
    "               epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(TRAINED_MODELS, benchmark.env_name)\n",
    "leap_net.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leap_net.visualize_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL_SAVE_PATH = get_path(EVALUATION_PATH, benchmark1)\n",
    "leapnet_metrics = benchmark.evaluate_simulator(augmented_simulator=leap_net,\n",
    "                                               eval_batch_size=100000,\n",
    "                                               dataset=\"all\",\n",
    "                                               shuffle=False,\n",
    "                                               save_path=None,\n",
    "                                               save_predictions=False\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leapnet_metrics[\"test\"][\"ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leapnet_metrics[\"test\"][\"Physics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leapnet_metrics[\"test_ood_topo\"][\"ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leapnet_metrics[\"test_ood_topo\"][\"Physics\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
