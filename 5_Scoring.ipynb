{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the global score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to demonstrate how to compute the global score to evaluate the performance for a given model (aka `AugmentedSimulator`). From now on, we assume all the inputs to compute the score are already available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceleration reference computation\n",
    "\n",
    "As the acceleration of simulation is one of the most important criteria in this competition, in this section we try to explain with respect to which reference the acceleration will be computed.\n",
    "\n",
    "#### Using Grid2op solver\n",
    "\n",
    "- The reference in this competition is the physical solver based on Newton Raphson optimisation which is implemented in [Grid2op](https://github.com/rte-france/Grid2Op) framework. It tries to solve the power flow equations in AC power system. We first provide a function which return the corresponding computation time for an indicated number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AC solver time (used as the reference) with respect to which the acceleration rate is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 49.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to solve one power flow:  0.00032784996554255483\n",
      "Time required to solve 100000 power flows:  32.784996554255486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lips.metrics.power_grid.compute_solver_time_grid2op import compute_solver_time_grid2op\n",
    "\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "grid2op_solver_time = compute_solver_time_grid2op(config_path=BENCH_CONFIG_PATH, benchmark_name=\"Benchmark_competition\", nb_samples=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Security Analysis\n",
    "\n",
    "However, a more optimized way to compute the power flow is through the security analysis which is based on the factorization of the decomposition of a matrix. This happens only during first step of power flow computation and allows to obtain a significant acceleration in comparison to the above mentioned approach. <span style=\"color:red\">In this competition, we use this optimized version as the reference power flow computation time to calculate the speed-ups of submissions.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.config import ConfigManager\n",
    "from lips.metrics.power_grid.compute_solver_time import compute_solver_time\n",
    "\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "config = ConfigManager(path=BENCH_CONFIG_PATH, section_name=\"Benchmark_competition\")\n",
    "\n",
    "sa_solver_time = compute_solver_time(nb_samples=int(1e5), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.68975707024849"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_solver_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acceleration obtained using Security Analysis is :  3.77 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"The acceleration obtained using Security Analysis is :  {(grid2op_solver_time / sa_solver_time):.2f} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input results description\n",
    "Hereafter, we provide the score computation procedure for the submissions. We start by an example of metrics returned by a baseline approach on `lips_idf_2023` environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\"ML\":{\"a_or\":0.02, # MAPE90 \n",
    "                      \"a_ex\":0.02, # MAPE90 \n",
    "                      \"p_or\":0.02, # MAPE90 \n",
    "                      \"p_ex\":0.02, # MAPE90 \n",
    "                      \"v_or\":1.49, # MAE\n",
    "                      \"v_ex\":1.28  # MAE\n",
    "                },\n",
    "                \"Physics\":{\n",
    "                      \"CURRENT_POS\": 0.2,\n",
    "                      \"VOLTAGE_POS\": 0.1,\n",
    "                      \"LOSS_POS\": 31.99,\n",
    "                      \"DISC_LINES\": 0,\n",
    "                      \"CHECK_LOSS\": 3.06,\n",
    "                      \"CHECK_GC\": 99.99,\n",
    "                      \"CHECK_LC\": 98.82,\n",
    "                      \"CHECK_JOULE_LAW\": 87.82                      \n",
    "                }\n",
    "               }\n",
    "\n",
    "test_ood_metrics = {\"ML\":{\"a_or\":0.03, # MAPE90 \n",
    "                          \"a_ex\":0.03, # MAPE90 \n",
    "                          \"p_or\":0.03, # MAPE90 \n",
    "                          \"p_ex\":0.03, # MAPE90 \n",
    "                          \"v_or\":2.61, # MAE\n",
    "                          \"v_ex\":2.27  # MAE\n",
    "                    },\n",
    "                    \"Physics\":{\n",
    "                          \"CURRENT_POS\": 0.4, # violation percentage (%)\n",
    "                          \"VOLTAGE_POS\": 0.1,\n",
    "                          \"LOSS_POS\": 34.94,\n",
    "                          \"DISC_LINES\": 0,\n",
    "                          \"CHECK_LOSS\": 6.61,\n",
    "                          \"CHECK_GC\": 99.99,\n",
    "                          \"CHECK_LC\": 98.95,\n",
    "                          \"CHECK_JOULE_LAW\": 89.59 \n",
    "                    }\n",
    "                   } \n",
    "\n",
    "speed_up = sa_solver_time / 2.65 # 2.65 is the inference time of a Neural Network based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the acceptability thresholds. Each variable is associated with 2 thresholds used to determine whether the result are great, acceptable or unacceptable and whether the result should be maximized or minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds={\"a_or\":(0.05,0.10,\"min\"),\n",
    "            \"a_ex\":(0.05,0.10,\"min\"),\n",
    "            \"p_or\":(0.05,0.10,\"min\"),\n",
    "            \"p_ex\":(0.05,0.10,\"min\"),\n",
    "            \"v_or\":(0.2,0.5,\"min\"),\n",
    "            \"v_ex\":(0.2,0.5,\"min\"),\n",
    "            \"CURRENT_POS\":(1., 5.,\"min\"),\n",
    "            \"VOLTAGE_POS\":(1.,5.,\"min\"),\n",
    "            \"LOSS_POS\":(1.,5.,\"min\"),\n",
    "            \"DISC_LINES\":(1.,5.,\"min\"),\n",
    "            \"CHECK_LOSS\":(1.,5.,\"min\"),\n",
    "            \"CHECK_GC\":(0.05,0.10,\"min\"),\n",
    "            \"CHECK_LC\":(0.05,0.10,\"min\"),\n",
    "            \"CHECK_JOULE_LAW\":(1.,5.,\"min\")\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, regarding the value obtained for the variable 'a_or'\n",
    "\n",
    "- if it is lower than 0.05, the result is great\n",
    "- if it is greater than 0.05 but lower than 0.10, the result is acceptable\n",
    "- if it is greater than 0.10, the result is not acceptable\n",
    "\n",
    "For a physical criteria `CHECK_GC` (check global conservation):\n",
    "\n",
    "- if the violation is less than 5 percent, the result is acceptable\n",
    "- if it is greater than 5 percent but lower than 10 percent, the result is acceptable\n",
    "- if it is greater than 10 percent, the result is not acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the configuration which are the coefficients considered for each category and subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration={\n",
    "    \"coefficients\":{\"test\":0.33, \"test_ood\":0.33, \"speed_up\":0.34},\n",
    "    \"test_ratio\":{\"ml\": 0.6, \"physics\":0.4},\n",
    "    \"test_ood_ratio\":{\"ml\": 0.6, \"physics\":0.4},\n",
    "    \"value_by_color\":{\"g\":2,\"o\":1,\"r\":0},\n",
    "    \"max_speed_ratio_allowed\":50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the result accuracy performances for all variables. We denote by:\n",
    "\n",
    "- g, a great result\n",
    "- o, an acceptable result\n",
    "- r, a not acceptable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ML': ['g', 'g', 'g', 'g', 'r', 'r'], 'Physics': ['g', 'g', 'r', 'g', 'o', 'r', 'r', 'r']}\n"
     ]
    }
   ],
   "source": [
    "results_test=dict()\n",
    "for subcategoryName, subcategoryVal in test_metrics.items():\n",
    "    results_test[subcategoryName]=[]\n",
    "    for variableName, variableError in subcategoryVal.items():\n",
    "        thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "        if evalType==\"min\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"g\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"r\"\n",
    "        elif evalType==\"max\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"r\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"g\"\n",
    "\n",
    "        results_test[subcategoryName].append(accuracyEval)\n",
    "    \n",
    "print(results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same for OOD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ML': ['g', 'g', 'g', 'g', 'r', 'r'], 'Physics': ['g', 'g', 'r', 'g', 'r', 'r', 'r', 'r']}\n"
     ]
    }
   ],
   "source": [
    "results_test_ood=dict()\n",
    "for subcategoryName, subcategoryVal in test_ood_metrics.items():\n",
    "    results_test_ood[subcategoryName]=[]\n",
    "    for variableName, variableError in subcategoryVal.items():\n",
    "        thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "        if evalType==\"min\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"g\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"r\"\n",
    "        elif evalType==\"max\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"r\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"g\"\n",
    "\n",
    "        results_test_ood[subcategoryName].append(accuracyEval)\n",
    "    \n",
    "print(results_test_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpeedMetric(speedUp,speedMax):\n",
    "    return max(min(math.log10(speedUp)/math.log10(speedMax),1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = configuration[\"coefficients\"]\n",
    "test_ratio = configuration[\"test_ratio\"]\n",
    "test_ood_ratio = configuration[\"test_ood_ratio\"]\n",
    "value_by_color = configuration[\"value_by_color\"]\n",
    "max_speed_ratio_allowed = configuration[\"max_speed_ratio_allowed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ml_subscore=0\n",
    "\n",
    "test_ml_res = sum([value_by_color[color] for color in results_test[\"ML\"]])\n",
    "test_ml_res = (test_ml_res * test_ratio[\"ml\"]) / (len(results_test[\"ML\"])*max(value_by_color.values()))\n",
    "test_ml_subscore += test_ml_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Physics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_physics_res = sum([value_by_color[color] for color in results_test[\"Physics\"]])\n",
    "test_physics_res = (test_physics_res*test_ratio[\"physics\"]) / (len(results_test[\"Physics\"])*max(value_by_color.values()))\n",
    "test_physics_subscore = test_physics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subscore = test_ml_subscore + test_physics_subscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_score = SpeedMetric(speedUp=speed_up,speedMax=max_speed_ratio_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30357320039998376"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speedup_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_ml_subscore=0\n",
    "\n",
    "test_ood_ml_res = sum([value_by_color[color] for color in results_test_ood[\"ML\"]])\n",
    "test_ood_ml_res = (test_ood_ml_res * test_ood_ratio[\"ml\"]) / (len(results_test_ood[\"ML\"])*max(value_by_color.values()))\n",
    "test_ood_ml_subscore += test_ood_ml_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39999999999999997"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ood_ml_subscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_physics_res = sum([value_by_color[color] for color in results_test_ood[\"Physics\"]])\n",
    "test_ood_physics_res = (test_ood_physics_res*test_ood_ratio[\"physics\"]) / (len(results_test_ood[\"Physics\"])*max(value_by_color.values()))\n",
    "test_ood_physics_subscore = test_ood_physics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15000000000000002"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ood_physics_subscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_subscore = test_ood_ml_subscore + test_ood_physics_subscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ood_subscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.44648881359945\n"
     ]
    }
   ],
   "source": [
    "globalScore=100*(coefficients[\"test\"]*test_subscore+coefficients[\"test_ood\"]*test_ood_subscore+coefficients[\"speed_up\"]*speedup_score)\n",
    "print(globalScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use the scoring function (available under `utils.compute_score`) to compute the score for already trained baseline models.\n",
    "\n",
    "<span style=\"color:red\"><b>This section is under construction for the moment ...</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required packages\n",
    "import os\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark\n",
    "\n",
    "#Define the required paths\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "DATA_PATH = os.path.join(\"input_data_local\", \"lips_idf_2023\")\n",
    "TRAINED_MODELS = os.path.join(\"input_data_local\", \"trained_models\")\n",
    "LOG_PATH = \"logs.log\"\n",
    "\n",
    "benchmark_kwargs = {\"attr_x\": (\"prod_p\", \"prod_v\", \"load_p\", \"load_q\"),\n",
    "                    \"attr_y\": (\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"v_or\", \"v_ex\"),\n",
    "                    \"attr_tau\": (\"line_status\", \"topo_vect\"),\n",
    "                    \"attr_physics\": None}\n",
    "\n",
    "benchmark = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                               config_path=BENCH_CONFIG_PATH,\n",
    "                               benchmark_name=\"Benchmark_competition\",\n",
    "                               load_data_set=True, \n",
    "                               load_ybus_as_sparse=False,\n",
    "                               log_path=LOG_PATH,\n",
    "                               **benchmark_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an already trained augmented simulator\n",
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_fc.ini\")\n",
    "\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark_competition\",\n",
    "                         bench_kwargs=benchmark_kwargs,\n",
    "                         sim_config_path=SIM_CONFIG_PATH,\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)\n",
    "\n",
    "LOAD_PATH = os.path.join(TRAINED_MODELS, \"lips_idf_2023\")\n",
    "tf_fc.restore(path=LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PATH = os.path.join(\"input_data_local\", \"eval_results\")\n",
    "metrics = benchmark.evaluate_simulator(augmented_simulator=tf_fc,\n",
    "                                       eval_batch_size=128,\n",
    "                                       dataset=\"all\",\n",
    "                                       shuffle=False,\n",
    "                                       save_path=EVALUATION_PATH,\n",
    "                                       save_predictions=False\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"test\"][\"ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_score import compute_global_score\n",
    "score = compute_global_score(metrics, benchmark.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "import utils.compute_score\n",
    "importlib.reload(utils.compute_score)\n",
    "from utils import compute_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_score.configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = compute_score.reconstruct_metric_dict(metrics, \"test\")\n",
    "test_ood_metrics = compute_score.reconstruct_metric_dict(metrics, \"test_ood_topo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_disc = compute_score.discretize_results(test_metrics)\n",
    "test_ood_results_disc = compute_score.discretize_results(test_ood_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeapNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_leapnet.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from lips.augmented_simulators.tensorflow_models.powergrid.leap_net import LeapNet\n",
    "from lips.dataset.scaler.powergrid_scaler import PowerGridScaler\n",
    "\n",
    "leap_net = LeapNet(name=\"tf_leapnet\",                  \n",
    "                   bench_config_path=BENCH_CONFIG_PATH,\n",
    "                   bench_config_name=\"Benchmark_competition\",\n",
    "                   bench_kwargs=benchmark_kwargs,\n",
    "                   sim_config_path=SIM_CONFIG_PATH,\n",
    "                   sim_config_name=\"DEFAULT\",\n",
    "                   scaler = PowerGridScaler,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "LOAD_PATH_TOPO_VECT = os.path.join(TRAINED_MODELS, \"lips_idf_2023\", leap_net.name)\n",
    "LOAD_PATH_TOPO_VECT = os.path.join(LOAD_PATH_TOPO_VECT, \"topo_vect_transformer.obj\")\n",
    "LOAD_PATH_TOPO_VECT = os.path.abspath(LOAD_PATH_TOPO_VECT)\n",
    "file_handler = open(LOAD_PATH_TOPO_VECT, \"rb\")\n",
    "topo_vect_transformer = pickle.load(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = os.path.join(TRAINED_MODELS, \"lips_idf_2023\")\n",
    "leap_net._topo_vect_transformer = topo_vect_transformer\n",
    "leap_net.restore(path=LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PATH = os.path.join(\"input_data_local\", \"eval_results\")\n",
    "leapnet_metrics = benchmark.evaluate_simulator(augmented_simulator=leap_net,\n",
    "                                               eval_batch_size=100000,\n",
    "                                               dataset=\"all\",\n",
    "                                               shuffle=False,\n",
    "                                               save_path=EVALUATION_PATH,\n",
    "                                               save_predictions=False\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_score import compute_global_score\n",
    "score = compute_global_score(leapnet_metrics, benchmark.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leapnet_metrics[\"test\"][\"ML\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lips_irt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
