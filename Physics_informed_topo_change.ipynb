{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark\n",
    "from utils.graph_utils_topo_change import prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some required pathes\n",
    "DATA_PATH = pathlib.Path().resolve() / \"input_data_local\" / \"lips_idf_2023\"\n",
    "BENCH_CONFIG_PATH = pathlib.Path().resolve() / \"configs\" / \"benchmarks\" / \"lips_idf_2023.ini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                               benchmark_name=\"Benchmark_competition_DC\",\n",
    "                               load_data_set=False, # we set False, as data are not yet generated\n",
    "                               config_path=BENCH_CONFIG_PATH,\n",
    "                               log_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data using DC solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.generate(nb_sample_train=int(2e3),\n",
    "                   nb_sample_val=int(2e3),\n",
    "                   nb_sample_test=int(2e3),\n",
    "                   nb_sample_test_ood_topo=int(2e3),\n",
    "                   store_as_sparse=True, #Â NOTE: Specific to envs with 118 or more nodes\n",
    "                   do_store_physics=True,\n",
    "                   is_dc=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(benchmark.train_dataset.size)\n",
    "print(benchmark.val_dataset.size)\n",
    "print(benchmark._test_dataset.size)\n",
    "print(benchmark._test_ood_topo_dataset.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") # or \"cuda:0\" if you have any GPU\n",
    "train_loader, val_loader, test_loader, test_ood_loader = prepare_dataset(benchmark=benchmark, batch_size=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.graph_utils_topo_change import GPGmodel_without_NN\n",
    "\n",
    "device = torch.device(\"cpu\") # select \"cuda:0\" if you have a GPU\n",
    "gpg_model_wo_nn = GPGmodel_without_NN(ref_node=68, num_gnn_layers=2000, device=device)\n",
    "gpg_model_wo_nn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions_list = []\n",
    "observations_list = []\n",
    "error_per_batch = []\n",
    "for batch in test_loader:\n",
    "    out, errors = gpg_model_wo_nn(batch)\n",
    "    predictions_list.append(out)\n",
    "    observations_list.append(batch.y)\n",
    "    error_per_batch.append([float(error.detach().cpu().numpy()) for error in errors])\n",
    "observations = torch.vstack(observations_list)\n",
    "predictions = torch.vstack(predictions_list)\n",
    "errors = np.vstack(error_per_batch)\n",
    "errors = errors.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(errors)\n",
    "plt.xlabel(\"# GNN layers (iterations)\")\n",
    "plt.ylabel(\"Local Conservation Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "predictions = predictions * (180/math.pi) * 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((predictions, observations), axis=1)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.metrics.ml_metrics.external_metrics import mape_quantile\n",
    "\n",
    "MAPE_10 = mape_quantile(y_true=observations.detach().cpu(), y_pred=predictions.detach().cpu(), quantile=0.9)\n",
    "print(\"MAPE 10: \", MAPE_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from utils.graph_utils import get_obs\n",
    "from utils.graph_utils_topo_change import get_all_active_powers\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "env, obs = get_obs(benchmark)\n",
    "p_ors_pred, p_exs_pred = get_all_active_powers(benchmark._test_dataset.data,\n",
    "                                               obs,\n",
    "                                               theta_bus=predictions.view(-1,obs.n_sub*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the predictions in a dictionary in order to that the LIPS framework could read them for evaluation\n",
    "my_predictions = {}\n",
    "my_predictions[\"p_or\"] = p_ors_pred\n",
    "my_predictions[\"p_ex\"] = p_exs_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE10_Power = mape_quantile(y_true=benchmark._test_dataset.data[\"p_or\"], \n",
    "                             y_pred=my_predictions[\"p_or\"],\n",
    "                             quantile=0.9)\n",
    "print(\"MAPE10 on Active Powers: \", MAPE10_Power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.metrics.power_grid.local_conservation import local_conservation\n",
    "\n",
    "LC_tolerance = 1e-2\n",
    "\n",
    "verification = local_conservation(predictions=my_predictions,\n",
    "                                  observations=benchmark._test_dataset.data,\n",
    "                                  tolerance=LC_tolerance,\n",
    "                                  env=env,\n",
    "                                  result_level=2)\n",
    "print(f\"Violation percentage: {verification['violation_percentage']:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lips_irt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
