{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the global score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to demonstrate how to compute the global score to evaluate the performance for a given model (aka `AugmentedSimulator`). This notebook is composed of three main parts:\n",
    "\n",
    "- [Acceleration reference computation](#acceleration) we show how the acceleration factor is obtained wrt the reference simulator\n",
    "- [Score computation step-by-step](#step-by-step) we disentangle the score computation throughout a real example\n",
    "- [Automatic Score Computation for local submissions](#auto-score) we provide a function which computes the score from a trained model or from saved metrics\n",
    "\n",
    "For more information concerning the computation of the score, the readers could refer to [this file](Evaluation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceleration reference computation <a id=\"acceleration\"></a>\n",
    "\n",
    "As the acceleration of simulation is one of the most important criteria in this competition, in this section we try to explain with respect to which reference the acceleration will be computed.\n",
    "\n",
    "#### Using AC Power Flow simulator (LightSim2Grid)\n",
    "\n",
    "- The reference in this competition is the physical solver based on Newton Raphson optimisation which is implemented in  [LightSim2Grid](https://github.com/BDonnot/lightsim2grid) and called through [Grid2op](https://github.com/rte-france/Grid2Op) framework. It solves the power flow equations in AC power system. We first provide a function which return the corresponding computation time for an indicated number of simulated samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AC solver time (used as the reference) with respect to which the acceleration rate is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 37.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to solve one power flow:  0.0003269047848880291\n",
      "Time required to solve 100000 power flows:  32.69047848880291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lips.metrics.power_grid.compute_solver_time_grid2op import compute_solver_time_grid2op\n",
    "\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "grid2op_solver_time = compute_solver_time_grid2op(config_path=BENCH_CONFIG_PATH, benchmark_name=\"Benchmark_competition\", nb_samples=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Security Analysis\n",
    "\n",
    "However, a more optimized way to compute the power flow for the risk assessment application is through the security analysis. It runs simulations on a grid state for every contingency of interest in terms of risk, more especially for every single line disconnection that could unexpectdly occur. It hence runs as many simulations as there are lines on the grid in a single computation. Compared to single powerflow computation, this is accelerated thanks to matrix decomposition and factorization that is only done once and reused all along the contingency simulations. \n",
    "\n",
    "This Security Analysis computation is used as a baseline for this risk assessment application of interest in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.config import ConfigManager\n",
    "from lips.metrics.power_grid.compute_solver_time import compute_solver_time\n",
    "\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "config = ConfigManager(path=BENCH_CONFIG_PATH, section_name=\"Benchmark_competition\")\n",
    "\n",
    "sa_solver_time = compute_solver_time(nb_samples=int(1e5), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.627058052987502"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_solver_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acceleration obtained using Security Analysis is :  3.77 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"The acceleration obtained using Security Analysis is :  {(grid2op_solver_time / sa_solver_time):.2f} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score computation step-by-step <a id='step-by-step'></a>\n",
    "Hereafter, we provide the score computation procedure for the submissions. We start by an example of metrics returned by a baseline approach on `lips_idf_2023` environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the metrics, you can refer to the LIPS paper Appendix.\n",
    "\n",
    "We yet highlight the use of a specific metric: the MAPE90. As in risk assessment, we are interested to detect overloads on the lines, we only compute the MAPE on for currents on the last 10% quantile.\n",
    "\n",
    "Once overloads are detected, an operator still needs to picture the flows on this state. Hence we assess the active power on the almost complete distribution, except the very low values which are not very significant. Hence the use of MAPE10 in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\"ML\":{\"a_or\":0.013, # MAPE90 \n",
    "                      \"a_ex\":0.013, # MAPE90 \n",
    "                      \"p_or\":0.029, # MAPE10 \n",
    "                      \"p_ex\":0.029, # MAPE10 \n",
    "                      \"v_or\":1.11, # MAE\n",
    "                      \"v_ex\":1.11  # MAE\n",
    "                },\n",
    "                \"Physics\":{\n",
    "                      \"CURRENT_POS\": 0.2,\n",
    "                      \"VOLTAGE_POS\": 0.1,\n",
    "                      \"LOSS_POS\": 27.88,\n",
    "                      \"DISC_LINES\": 100.0,\n",
    "                      \"CHECK_LOSS\": 1.54,\n",
    "                      \"CHECK_GC\": 99.99,\n",
    "                      \"CHECK_LC\": 98.55,\n",
    "                      \"CHECK_JOULE_LAW\": 85.84                      \n",
    "                }\n",
    "               }\n",
    "\n",
    "test_ood_metrics = {\"ML\":{\"a_or\":0.024, # MAPE90 \n",
    "                          \"a_ex\":0.024, # MAPE90 \n",
    "                          \"p_or\":0.041, # MAPE10 \n",
    "                          \"p_ex\":0.040, # MAPE10 \n",
    "                          \"v_or\":2.28, # MAE\n",
    "                          \"v_ex\":2.07  # MAE\n",
    "                    },\n",
    "                    \"Physics\":{\n",
    "                          \"CURRENT_POS\": 0.4, # violation percentage (%)\n",
    "                          \"VOLTAGE_POS\": 0.1,\n",
    "                          \"LOSS_POS\": 30.88,\n",
    "                          \"DISC_LINES\": 100.0,\n",
    "                          \"CHECK_LOSS\": 4.63,\n",
    "                          \"CHECK_GC\": 99.99,\n",
    "                          \"CHECK_LC\": 98.70,\n",
    "                          \"CHECK_JOULE_LAW\": 88.48 \n",
    "                    }\n",
    "                   } \n",
    "\n",
    "speed_up = grid2op_solver_time / 2.74 # 2.74 is the inference time of a Neural Network based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the acceptability thresholds. Each variable is associated with 2 thresholds used to determine whether the result are great, acceptable or unacceptable and whether the result should be maximized or minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds={\"a_or\":(0.02,0.05,\"min\"),\n",
    "            \"a_ex\":(0.02,0.05,\"min\"),\n",
    "            \"p_or\":(0.02,0.05,\"min\"),\n",
    "            \"p_ex\":(0.02,0.05,\"min\"),\n",
    "            \"v_or\":(0.2,0.5,\"min\"),\n",
    "            \"v_ex\":(0.2,0.5,\"min\"),\n",
    "            \"CURRENT_POS\":(1., 5.,\"min\"),\n",
    "            \"VOLTAGE_POS\":(1.,5.,\"min\"),\n",
    "            \"LOSS_POS\":(1.,5.,\"min\"),\n",
    "            \"DISC_LINES\":(1.,5.,\"min\"),\n",
    "            \"CHECK_LOSS\":(1.,5.,\"min\"),\n",
    "            \"CHECK_GC\":(0.05,0.10,\"min\"),\n",
    "            \"CHECK_LC\":(0.05,0.10,\"min\"),\n",
    "            \"CHECK_JOULE_LAW\":(1.,5.,\"min\")\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, regarding the value obtained for the variable 'a_or'\n",
    "\n",
    "- if it is lower than 2%, the result is great\n",
    "- if it is greater than 2% but lower than 5%, the result is acceptable\n",
    "- if it is greater than 5%, the result is not acceptable\n",
    "\n",
    "For a physical criteria `CHECK_GC` (check global conservation):\n",
    "\n",
    "- if the violation is less than 5 percent, the result is acceptable\n",
    "- if it is greater than 5 percent but lower than 10 percent, the result is acceptable\n",
    "- if it is greater than 10 percent, the result is not acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the configuration which are the coefficients considered for each category and subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration={\n",
    "    \"coefficients\":{\"test\":0.3, \"test_ood\":0.3, \"speed_up\":0.4},\n",
    "    \"test_ratio\":{\"ml\": 0.66, \"physics\":0.34},\n",
    "    \"test_ood_ratio\":{\"ml\": 0.66, \"physics\":0.34},\n",
    "    \"value_by_color\":{\"g\":2,\"o\":1,\"r\":0},\n",
    "    \"max_speed_ratio_allowed\":50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the result accuracy performances for all variables. We denote by:\n",
    "\n",
    "- g, a great result\n",
    "- o, an acceptable result\n",
    "- r, a not acceptable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ML': ['g', 'g', 'o', 'o', 'r', 'r'], 'Physics': ['g', 'g', 'r', 'r', 'o', 'r', 'r', 'r']}\n"
     ]
    }
   ],
   "source": [
    "results_test=dict()\n",
    "for subcategoryName, subcategoryVal in test_metrics.items():\n",
    "    results_test[subcategoryName]=[]\n",
    "    for variableName, variableError in subcategoryVal.items():\n",
    "        thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "        if evalType==\"min\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"g\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"r\"\n",
    "        elif evalType==\"max\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"r\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"g\"\n",
    "\n",
    "        results_test[subcategoryName].append(accuracyEval)\n",
    "    \n",
    "print(results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same for OOD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ML': ['o', 'o', 'o', 'o', 'r', 'r'], 'Physics': ['g', 'g', 'r', 'r', 'o', 'r', 'r', 'r']}\n"
     ]
    }
   ],
   "source": [
    "results_test_ood=dict()\n",
    "for subcategoryName, subcategoryVal in test_ood_metrics.items():\n",
    "    results_test_ood[subcategoryName]=[]\n",
    "    for variableName, variableError in subcategoryVal.items():\n",
    "        thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "        if evalType==\"min\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"g\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"r\"\n",
    "        elif evalType==\"max\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"r\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"g\"\n",
    "\n",
    "        results_test_ood[subcategoryName].append(accuracyEval)\n",
    "    \n",
    "print(results_test_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_function(x, a, b, c, k):\n",
    "    if x == 1.:\n",
    "        return 0.\n",
    "    else:\n",
    "        return a*(x**2) + b*x + c + math.log10(k*x)\n",
    "\n",
    "def SpeedMetric(speedUp, speedMax):\n",
    "    a=0.01 # 0.01\n",
    "    b=0.5 #0.5\n",
    "    c=0.1 #0.1\n",
    "    k=9\n",
    "    res = quadratic_function(speedUp, a=a, b=b, c=c, k=k) / quadratic_function(speedMax, a=a, b=b, c=c, k=k)\n",
    "    return max(min(res, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = configuration[\"coefficients\"]\n",
    "test_ratio = configuration[\"test_ratio\"]\n",
    "test_ood_ratio = configuration[\"test_ood_ratio\"]\n",
    "value_by_color = configuration[\"value_by_color\"]\n",
    "max_speed_ratio_allowed = configuration[\"max_speed_ratio_allowed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ml_subscore=0\n",
    "\n",
    "test_ml_res = sum([value_by_color[color] for color in results_test[\"ML\"]])\n",
    "test_ml_res = (test_ml_res * test_ratio[\"ml\"]) / (len(results_test[\"ML\"])*max(value_by_color.values()))\n",
    "test_ml_subscore += test_ml_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Physics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_physics_res = sum([value_by_color[color] for color in results_test[\"Physics\"]])\n",
    "test_physics_res = (test_physics_res*test_ratio[\"physics\"]) / (len(results_test[\"Physics\"])*max(value_by_color.values()))\n",
    "test_physics_subscore = test_physics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset subscore (ML + Physics) : 0.44\n"
     ]
    }
   ],
   "source": [
    "test_subscore = test_ml_subscore + test_physics_subscore\n",
    "print(f\"Test dataset subscore (ML + Physics) : {test_subscore:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed-up score: 0.18\n"
     ]
    }
   ],
   "source": [
    "speedup_score = SpeedMetric(speedUp=speed_up,speedMax=max_speed_ratio_allowed)\n",
    "print(f\"Speed-up score: {speedup_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_ml_subscore=0\n",
    "\n",
    "test_ood_ml_res = sum([value_by_color[color] for color in results_test_ood[\"ML\"]])\n",
    "test_ood_ml_res = (test_ood_ml_res * test_ood_ratio[\"ml\"]) / (len(results_test_ood[\"ML\"])*max(value_by_color.values()))\n",
    "test_ood_ml_subscore += test_ood_ml_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_physics_res = sum([value_by_color[color] for color in results_test_ood[\"Physics\"]])\n",
    "test_ood_physics_res = (test_ood_physics_res*test_ood_ratio[\"physics\"]) / (len(results_test_ood[\"Physics\"])*max(value_by_color.values()))\n",
    "test_ood_physics_subscore = test_ood_physics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOD dataset score (ML + Physics):  0.33\n"
     ]
    }
   ],
   "source": [
    "test_ood_subscore = test_ood_ml_subscore + test_ood_physics_subscore\n",
    "print(f\"OOD dataset score (ML + Physics):  {test_ood_subscore:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.093348207718172\n"
     ]
    }
   ],
   "source": [
    "globalScore=100*(coefficients[\"test\"]*test_subscore+coefficients[\"test_ood\"]*test_ood_subscore+coefficients[\"speed_up\"]*speedup_score)\n",
    "print(globalScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Score Computation for local submissions <a id='auto-score'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use the scoring function (available under `utils.compute_score`) in two ways:  \n",
    "- to compute the score for already trained baseline models;\n",
    "- to compute the score on the basis of the saved evaluation results (dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the score using an already trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate an already trained baseline (a fully connected architecture) and get the corresponding score using `compute_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required packages\n",
    "import os\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark\n",
    "\n",
    "#Define the required paths\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "DATA_PATH = os.path.join(\"input_data_local\", \"lips_idf_2023\")\n",
    "TRAINED_MODELS = os.path.join(\"input_data_local\", \"trained_models\")\n",
    "LOG_PATH = \"logs.log\"\n",
    "\n",
    "benchmark_kwargs = {\"attr_x\": (\"prod_p\", \"prod_v\", \"load_p\", \"load_q\"),\n",
    "                    \"attr_y\": (\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"v_or\", \"v_ex\"),\n",
    "                    \"attr_tau\": (\"line_status\", \"topo_vect\"),\n",
    "                    \"attr_physics\": None}\n",
    "\n",
    "benchmark = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                               config_path=BENCH_CONFIG_PATH,\n",
    "                               benchmark_name=\"Benchmark_competition\",\n",
    "                               load_data_set=True, \n",
    "                               log_path=LOG_PATH,\n",
    "                               **benchmark_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "memory_limit = 20000\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an already trained augmented simulator\n",
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_fc.ini\")\n",
    "\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark_competition\",\n",
    "                         bench_kwargs=benchmark_kwargs,\n",
    "                         sim_config_path=SIM_CONFIG_PATH,\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)\n",
    "\n",
    "LOAD_PATH = os.path.join(TRAINED_MODELS, \"lips_idf_2023\")\n",
    "tf_fc.restore(path=LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PATH = os.path.join(\"input_data_local\", \"eval_results\", \"lips_idf_2023\")\n",
    "metrics = benchmark.evaluate_simulator(augmented_simulator=tf_fc,\n",
    "                                       eval_batch_size=128,\n",
    "                                       dataset=\"all\",\n",
    "                                       shuffle=False,\n",
    "                                       save_path=EVALUATION_PATH,\n",
    "                                       save_predictions=False\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 50.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to solve one power flow:  0.0003298692740499973\n",
      "Time required to solve 100000 power flows:  32.98692740499973\n",
      "22.43482332440552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.compute_score import compute_global_score\n",
    "score = compute_global_score(metrics, benchmark.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the score using the saved evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the evaluation results for the baseline architecture (LeapNet architecture in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def import_metrics(path, dataset):\n",
    "    path_to_results = os.path.join(path, dataset, \"eval_res.json\")\n",
    "    with open(path_to_results) as json_file:\n",
    "        metrics = json.load(json_file)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PATH = os.path.join(\"input_data_local\", \"eval_results\", \"lips_idf_2023\", \"tf_leapnet_DEFAULT\")\n",
    "metrics_dict = dict()\n",
    "metrics_dict[\"test\"] = import_metrics(EVALUATION_PATH, \"test\")\n",
    "metrics_dict[\"test_ood_topo\"] = import_metrics(EVALUATION_PATH, \"test_ood_topo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 49.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to solve one power flow:  0.000327156824991107\n",
      "Time required to solve 100000 power flows:  32.7156824991107\n",
      "30.083020095673813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.compute_score import compute_global_score\n",
    "score = compute_global_score(metrics_dict, benchmark.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the physical solvers scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_score import compute_ml_subscore, compute_physics_subscore, SpeedMetric, configuration\n",
    "\n",
    "test_results_disc = {\"ML\": ['g','g','g','g','g','g'], \"Physics\": ['g','g','g','g','g','g','g','g']}\n",
    "test_ood_results_disc = {\"ML\": ['g','g','g','g','g','g'], \"Physics\": ['g','g','g','g','g','g','g','g']}\n",
    "coefficients = configuration[\"coefficients\"]\n",
    "max_speed_ratio_allowed = configuration[\"max_speed_ratio_allowed\"]\n",
    "\n",
    "test_ml_subscore = compute_ml_subscore(test_results_disc, key=\"test_ratio\")\n",
    "test_physics_subscore = compute_physics_subscore(test_results_disc, key=\"test_ratio\")\n",
    "test_subscore = test_ml_subscore + test_physics_subscore\n",
    "\n",
    "test_ood_ml_subscore = compute_ml_subscore(test_ood_results_disc, key=\"test_ood_ratio\")\n",
    "test_ood_physics_subscore = compute_physics_subscore(test_ood_results_disc, key=\"test_ood_ratio\")\n",
    "test_ood_subscore = test_ood_ml_subscore + test_ood_physics_subscore\n",
    "\n",
    "speed_up = 1. # LighSim2grid\n",
    "# speed_up = 3.77 # Security Analysis\n",
    "\n",
    "speedup_score = SpeedMetric(speedUp=speed_up, speedMax=max_speed_ratio_allowed)\n",
    "\n",
    "globalScore = 100*(coefficients[\"test\"]*test_subscore+coefficients[\"test_ood\"]*test_ood_subscore+coefficients[\"speed_up\"]*speedup_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark table\n",
    "The table provided below shows the benchmark results for the evaluations made in the previous sections of this notebook. Two first lines are physical solvers, which have the exact solutions for all the variables and are used as the ground truth for comparisons. However, their accelerations are not enough to obtain a global score of 100%. On the other hand, Fully Connected and LeapNet are two baselines based on Neural Networks. We can see that they do not show satisfying results regarding most of the considered criteria. The LeapNet architecture shows better generalization properties in comparison to Fully Connected architecture due to its specific design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Benchmark table](img/Benchmark_table_new.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
