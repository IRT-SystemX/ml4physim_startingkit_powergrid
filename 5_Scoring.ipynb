{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the global score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to demonstrate how to compute the global score to evaluate the performance for a given model (aka `AugmentedSimulator`). This notebook is composed of three main parts:\n",
    "\n",
    "- [Acceleration reference computation](#acceleration) we show how the acceleration factor is obtained wrt the reference simulator\n",
    "- [Score computation step-by-step](#step-by-step) we disentangle the score computation throughout a real example\n",
    "- [Automatic Score Computation for local submissions](#auto-score) we provide a function which computes the score from a trained model or from saved metrics\n",
    "\n",
    "For more information concerning the computation of the score, the readers could refer to [this file](Evaluation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceleration reference computation <a id=\"acceleration\"></a>\n",
    "\n",
    "As the acceleration of simulation is one of the most important criteria in this competition, in this section we try to explain with respect to which reference the acceleration will be computed.\n",
    "\n",
    "#### Using Grid2op solver\n",
    "\n",
    "- The reference in this competition is the physical solver based on Newton Raphson optimisation which is implemented in [Grid2op](https://github.com/rte-france/Grid2Op) framework. It tries to solve the power flow equations in AC power system. We first provide a function which return the corresponding computation time for an indicated number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the AC solver time (used as the reference) with respect to which the acceleration rate is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 49.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required to solve one power flow:  0.00032784996554255483\n",
      "Time required to solve 100000 power flows:  32.784996554255486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lips.metrics.power_grid.compute_solver_time_grid2op import compute_solver_time_grid2op\n",
    "\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "grid2op_solver_time = compute_solver_time_grid2op(config_path=BENCH_CONFIG_PATH, benchmark_name=\"Benchmark_competition\", nb_samples=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Security Analysis\n",
    "\n",
    "However, a more optimized way to compute the power flow is through the security analysis which is based on the factorization of the decomposition of a matrix. This happens only during first step of power flow computation and allows to obtain a significant acceleration in comparison to the above mentioned approach. <span style=\"color:red\">In this competition, we use this optimized version as the reference power flow computation time to calculate the speed-ups of submissions.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lips.config import ConfigManager\n",
    "from lips.metrics.power_grid.compute_solver_time import compute_solver_time\n",
    "\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "config = ConfigManager(path=BENCH_CONFIG_PATH, section_name=\"Benchmark_competition\")\n",
    "\n",
    "sa_solver_time = compute_solver_time(nb_samples=int(1e5), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.627058052987502"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_solver_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acceleration obtained using Security Analysis is :  3.77 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"The acceleration obtained using Security Analysis is :  {(grid2op_solver_time / sa_solver_time):.2f} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score computation step-by-step <a id='step-by-step'></a>\n",
    "Hereafter, we provide the score computation procedure for the submissions. We start by an example of metrics returned by a baseline approach on `lips_idf_2023` environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = {\"ML\":{\"a_or\":0.02, # MAPE90 \n",
    "                      \"a_ex\":0.02, # MAPE90 \n",
    "                      \"p_or\":0.02, # MAPE90 \n",
    "                      \"p_ex\":0.02, # MAPE90 \n",
    "                      \"v_or\":1.49, # MAE\n",
    "                      \"v_ex\":1.28  # MAE\n",
    "                },\n",
    "                \"Physics\":{\n",
    "                      \"CURRENT_POS\": 0.2,\n",
    "                      \"VOLTAGE_POS\": 0.1,\n",
    "                      \"LOSS_POS\": 31.99,\n",
    "                      \"DISC_LINES\": 0,\n",
    "                      \"CHECK_LOSS\": 3.06,\n",
    "                      \"CHECK_GC\": 99.99,\n",
    "                      \"CHECK_LC\": 98.82,\n",
    "                      \"CHECK_JOULE_LAW\": 87.82                      \n",
    "                }\n",
    "               }\n",
    "\n",
    "test_ood_metrics = {\"ML\":{\"a_or\":0.03, # MAPE90 \n",
    "                          \"a_ex\":0.03, # MAPE90 \n",
    "                          \"p_or\":0.03, # MAPE90 \n",
    "                          \"p_ex\":0.03, # MAPE90 \n",
    "                          \"v_or\":2.61, # MAE\n",
    "                          \"v_ex\":2.27  # MAE\n",
    "                    },\n",
    "                    \"Physics\":{\n",
    "                          \"CURRENT_POS\": 0.4, # violation percentage (%)\n",
    "                          \"VOLTAGE_POS\": 0.1,\n",
    "                          \"LOSS_POS\": 34.94,\n",
    "                          \"DISC_LINES\": 0,\n",
    "                          \"CHECK_LOSS\": 6.61,\n",
    "                          \"CHECK_GC\": 99.99,\n",
    "                          \"CHECK_LC\": 98.95,\n",
    "                          \"CHECK_JOULE_LAW\": 89.59 \n",
    "                    }\n",
    "                   } \n",
    "\n",
    "speed_up = sa_solver_time / 3.34 # 3.34 is the inference time of a Neural Network based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the acceptability thresholds. Each variable is associated with 2 thresholds used to determine whether the result are great, acceptable or unacceptable and whether the result should be maximized or minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds={\"a_or\":(0.05,0.10,\"min\"),\n",
    "            \"a_ex\":(0.05,0.10,\"min\"),\n",
    "            \"p_or\":(0.05,0.10,\"min\"),\n",
    "            \"p_ex\":(0.05,0.10,\"min\"),\n",
    "            \"v_or\":(0.2,0.5,\"min\"),\n",
    "            \"v_ex\":(0.2,0.5,\"min\"),\n",
    "            \"CURRENT_POS\":(1., 5.,\"min\"),\n",
    "            \"VOLTAGE_POS\":(1.,5.,\"min\"),\n",
    "            \"LOSS_POS\":(1.,5.,\"min\"),\n",
    "            \"DISC_LINES\":(1.,5.,\"min\"),\n",
    "            \"CHECK_LOSS\":(1.,5.,\"min\"),\n",
    "            \"CHECK_GC\":(0.05,0.10,\"min\"),\n",
    "            \"CHECK_LC\":(0.05,0.10,\"min\"),\n",
    "            \"CHECK_JOULE_LAW\":(1.,5.,\"min\")\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, regarding the value obtained for the variable 'a_or'\n",
    "\n",
    "- if it is lower than 0.05, the result is great\n",
    "- if it is greater than 0.05 but lower than 0.10, the result is acceptable\n",
    "- if it is greater than 0.10, the result is not acceptable\n",
    "\n",
    "For a physical criteria `CHECK_GC` (check global conservation):\n",
    "\n",
    "- if the violation is less than 5 percent, the result is acceptable\n",
    "- if it is greater than 5 percent but lower than 10 percent, the result is acceptable\n",
    "- if it is greater than 10 percent, the result is not acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the configuration which are the coefficients considered for each category and subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration={\n",
    "    \"coefficients\":{\"test\":0.33, \"test_ood\":0.33, \"speed_up\":0.34},\n",
    "    \"test_ratio\":{\"ml\": 0.6, \"physics\":0.4},\n",
    "    \"test_ood_ratio\":{\"ml\": 0.6, \"physics\":0.4},\n",
    "    \"value_by_color\":{\"g\":2,\"o\":1,\"r\":0},\n",
    "    \"max_speed_ratio_allowed\":50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the result accuracy performances for all variables. We denote by:\n",
    "\n",
    "- g, a great result\n",
    "- o, an acceptable result\n",
    "- r, a not acceptable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ML': ['g', 'g', 'g', 'g', 'r', 'r'], 'Physics': ['g', 'g', 'r', 'g', 'o', 'r', 'r', 'r']}\n"
     ]
    }
   ],
   "source": [
    "results_test=dict()\n",
    "for subcategoryName, subcategoryVal in test_metrics.items():\n",
    "    results_test[subcategoryName]=[]\n",
    "    for variableName, variableError in subcategoryVal.items():\n",
    "        thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "        if evalType==\"min\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"g\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"r\"\n",
    "        elif evalType==\"max\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"r\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"g\"\n",
    "\n",
    "        results_test[subcategoryName].append(accuracyEval)\n",
    "    \n",
    "print(results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same for OOD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ML': ['g', 'g', 'g', 'g', 'r', 'r'], 'Physics': ['g', 'g', 'r', 'g', 'r', 'r', 'r', 'r']}\n"
     ]
    }
   ],
   "source": [
    "results_test_ood=dict()\n",
    "for subcategoryName, subcategoryVal in test_ood_metrics.items():\n",
    "    results_test_ood[subcategoryName]=[]\n",
    "    for variableName, variableError in subcategoryVal.items():\n",
    "        thresholdMin,thresholdMax,evalType=thresholds[variableName]\n",
    "        if evalType==\"min\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"g\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"r\"\n",
    "        elif evalType==\"max\":\n",
    "            if variableError<thresholdMin:\n",
    "                accuracyEval=\"r\"\n",
    "            elif thresholdMin<variableError<thresholdMax:\n",
    "                accuracyEval=\"o\"\n",
    "            else:\n",
    "                accuracyEval=\"g\"\n",
    "\n",
    "        results_test_ood[subcategoryName].append(accuracyEval)\n",
    "    \n",
    "print(results_test_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpeedMetric(speedUp,speedMax):\n",
    "    return max(min(math.log10(speedUp)/math.log10(speedMax),1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = configuration[\"coefficients\"]\n",
    "test_ratio = configuration[\"test_ratio\"]\n",
    "test_ood_ratio = configuration[\"test_ood_ratio\"]\n",
    "value_by_color = configuration[\"value_by_color\"]\n",
    "max_speed_ratio_allowed = configuration[\"max_speed_ratio_allowed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ml_subscore=0\n",
    "\n",
    "test_ml_res = sum([value_by_color[color] for color in results_test[\"ML\"]])\n",
    "test_ml_res = (test_ml_res * test_ratio[\"ml\"]) / (len(results_test[\"ML\"])*max(value_by_color.values()))\n",
    "test_ml_subscore += test_ml_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Physics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_physics_res = sum([value_by_color[color] for color in results_test[\"Physics\"]])\n",
    "test_physics_res = (test_physics_res*test_ratio[\"physics\"]) / (len(results_test[\"Physics\"])*max(value_by_color.values()))\n",
    "test_physics_subscore = test_physics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subscore = test_ml_subscore + test_physics_subscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_subscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_score = SpeedMetric(speedUp=speed_up,speedMax=max_speed_ratio_allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2425682929234455"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speedup_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_ml_subscore=0\n",
    "\n",
    "test_ood_ml_res = sum([value_by_color[color] for color in results_test_ood[\"ML\"]])\n",
    "test_ood_ml_res = (test_ood_ml_res * test_ood_ratio[\"ml\"]) / (len(results_test_ood[\"ML\"])*max(value_by_color.values()))\n",
    "test_ood_ml_subscore += test_ood_ml_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39999999999999997"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ood_ml_subscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_physics_res = sum([value_by_color[color] for color in results_test_ood[\"Physics\"]])\n",
    "test_ood_physics_res = (test_ood_physics_res*test_ood_ratio[\"physics\"]) / (len(results_test_ood[\"Physics\"])*max(value_by_color.values()))\n",
    "test_ood_physics_subscore = test_ood_physics_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15000000000000002"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ood_physics_subscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ood_subscore = test_ood_ml_subscore + test_ood_physics_subscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ood_subscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.37232195939715\n"
     ]
    }
   ],
   "source": [
    "globalScore=100*(coefficients[\"test\"]*test_subscore+coefficients[\"test_ood\"]*test_ood_subscore+coefficients[\"speed_up\"]*speedup_score)\n",
    "print(globalScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Score Computation for local submissions <a id='auto-score'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use the scoring function (available under `utils.compute_score`) in two ways:  \n",
    "- to compute the score for already trained baseline models;\n",
    "- to compute the score on the basis of the saved evaluation results (dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the score using an already trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate an already trained baseline (a fully connected architecture) and get the corresponding score using `compute_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import required packages\n",
    "import os\n",
    "from lips.benchmark.powergridBenchmark import PowerGridBenchmark\n",
    "\n",
    "#Define the required paths\n",
    "BENCH_CONFIG_PATH = os.path.join(\"configs\", \"benchmarks\", \"lips_idf_2023.ini\")\n",
    "DATA_PATH = os.path.join(\"input_data_local\", \"lips_idf_2023\")\n",
    "TRAINED_MODELS = os.path.join(\"input_data_local\", \"trained_models\")\n",
    "LOG_PATH = \"logs.log\"\n",
    "\n",
    "benchmark_kwargs = {\"attr_x\": (\"prod_p\", \"prod_v\", \"load_p\", \"load_q\"),\n",
    "                    \"attr_y\": (\"a_or\", \"a_ex\", \"p_or\", \"p_ex\", \"v_or\", \"v_ex\"),\n",
    "                    \"attr_tau\": (\"line_status\", \"topo_vect\"),\n",
    "                    \"attr_physics\": None}\n",
    "\n",
    "benchmark = PowerGridBenchmark(benchmark_path=DATA_PATH,\n",
    "                               config_path=BENCH_CONFIG_PATH,\n",
    "                               benchmark_name=\"Benchmark_competition\",\n",
    "                               load_data_set=True, \n",
    "                               load_ybus_as_sparse=False,\n",
    "                               log_path=LOG_PATH,\n",
    "                               **benchmark_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an already trained augmented simulator\n",
    "from lips.augmented_simulators.tensorflow_models import TfFullyConnected\n",
    "from lips.dataset.scaler import StandardScaler\n",
    "\n",
    "# Indicate the path required for corresponding augmented simulator parameters\n",
    "SIM_CONFIG_PATH = os.path.join(\"configs\", \"simulators\", \"tf_fc.ini\")\n",
    "\n",
    "tf_fc = TfFullyConnected(name=\"tf_fc\",\n",
    "                         bench_config_path=BENCH_CONFIG_PATH,\n",
    "                         bench_config_name=\"Benchmark_competition\",\n",
    "                         bench_kwargs=benchmark_kwargs,\n",
    "                         sim_config_path=SIM_CONFIG_PATH,\n",
    "                         sim_config_name=\"DEFAULT\",\n",
    "                         scaler=StandardScaler,\n",
    "                         log_path=LOG_PATH)\n",
    "\n",
    "LOAD_PATH = os.path.join(TRAINED_MODELS, \"lips_idf_2023\")\n",
    "tf_fc.restore(path=LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PATH = os.path.join(\"input_data_local\", \"eval_results\", \"lips_idf_2023\")\n",
    "metrics = benchmark.evaluate_simulator(augmented_simulator=tf_fc,\n",
    "                                       eval_batch_size=128,\n",
    "                                       dataset=\"all\",\n",
    "                                       shuffle=False,\n",
    "                                       save_path=EVALUATION_PATH,\n",
    "                                       save_predictions=False\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.90493509272275\n"
     ]
    }
   ],
   "source": [
    "from utils.compute_score import compute_global_score\n",
    "score = compute_global_score(metrics, benchmark.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coefficients': {'test': 0.33, 'test_ood': 0.33, 'speed_up': 0.34},\n",
       " 'test_ratio': {'ml': 0.6, 'physics': 0.4},\n",
       " 'test_ood_ratio': {'ml': 0.6, 'physics': 0.4},\n",
       " 'value_by_color': {'g': 2, 'o': 1, 'r': 0},\n",
       " 'max_speed_ratio_allowed': 50}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score.configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the score using the saved evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the evaluation results for the baseline architecture (LeapNet architecture in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def import_metrics(path, dataset):\n",
    "    path_to_results = os.path.join(path, dataset, \"eval_res.json\")\n",
    "    with open(path_to_results) as json_file:\n",
    "        metrics = json.load(json_file)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PATH = os.path.join(\"input_data_local\", \"eval_results\", \"lips_idf_2023\", \"tf_leapnet_DEFAULT\")\n",
    "metrics_dict = dict()\n",
    "metrics_dict[\"test\"] = import_metrics(EVALUATION_PATH, \"test\")\n",
    "metrics_dict[\"test_ood_topo\"] = import_metrics(EVALUATION_PATH, \"test_ood_topo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.35264931559977\n"
     ]
    }
   ],
   "source": [
    "from utils.compute_score import compute_global_score\n",
    "score = compute_global_score(metrics_dict, benchmark.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lips_irt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
